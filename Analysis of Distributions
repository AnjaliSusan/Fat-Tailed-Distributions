\documentclass[14pt, a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\theoremstyle{definition}
\newtheorem*{definition}{Definition}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}


\title{What are fat tails 
and their consequences?
\\ Honours Notes \\ Anjali Susan Oommen}
\date{}

\begin{document}

\maketitle

\section{Normal Distribution}

\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }& $\mu \in \R$ (mean) (location parameter) \\ & $\sigma^2 \in \R_{>0}$ (variance) \\
 \hline
 \textbf{Support} & $x \in \R$ \\
 \hline
 \textbf{Probability Mass Function} & $\frac{1}{\sigma \sqrt{2 \pi } } e^{ - \frac{1}{2} \big( \frac{x-\mu}{\sigma} \big) ^2 } $  \\
 \hline
 \textbf{Cumulative Distribution Function} & $\frac{1}{2} \Bigg[1 + erf(\frac{x - \mu}{\sigma \sqrt{2}}  ) \Bigg]$ \\
 \hline
 \textbf{Mean} & $\mu$ \\ 
 \hline
 \textbf{Variance} & $\sigma^2$ \\
 \hline
 \textbf{Median} & $\mu$ \\ 
 \hline
 \textbf{Mode} & $\mu$ \\
 \hline
 \textbf{Moment Generating Function} & exp$(\mu t + \frac{\mu^2 t^2}{2})$ \\
 \hline
 \textbf{Skewness} & $0$  \\
 \hline
 \textbf{Excess Kurtosis} & $3(\sigma^4 -1)$ \\
 \hline
       
  \end{tabular}
  
  
Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve. While normal distributions are symmetric, but not all symmetric distributions are normal. In a normal distribution, mean = median = mode. It is also unimodal in nature. 

It is a type of continuous probability distribution for a real-valued random variable. The general form of its probability density function is 
\[ f(x) = \frac{1}{\sigma \sqrt{2 \pi } } e^{ - \frac{1}{2} \big( \frac{x-\mu}{\sigma} \big) ^2 }  \]
where $\mu$ is the mean of the distribution and $\sigma$ is the standard deviation.

For a normal distribution, $68$ percent of the observations are within $+/-$ one standard deviation of the mean, $95$ percent are within $+/-$ two standard deviations, and $99.7$ percent are within $+/-$ three standard deviations.

\textbf{To find Skewness}
\[ \mu_3= \frac{E(X^3) - 3 \mu \sigma^2 - \mu^3}{ \sigma^3} \]


\textbf{To find Kurtosis}
\[ \mu_4 = \frac{ E(X^4) - 4 \mu E(X^3) + 6 \mu^2 E(X^2) - 3 \mu^4}{ \sigma^4} \]

\subsection{Standard Normal Distribution}
In a standard normal distribution the mean is zero and the standard deviation is 1. It has zero skew and a kurtosis of 3. Every normal distribution can be standardised. A normal dist is a standard normal distribution stretched by a the standard deviation of the normal distribution and translated by the mean of the normal distribution. The general probability density function of a standard normal distribution is 
\[ f(x) = \frac{e^{-x^2 / 2}}{\sqrt{2 \pi} } \]

A normal distribution can be standardised by giving the observations in the dataset of the distribution a z-score 
\[ z = \frac{x - \mu}{\sigma}\]

\section{Bernoulli Distribution}


\begin{tabular}{|c|c|}  %no. of columns
\hline
\textbf{Parameters }&  $p \in [0,1]$ \\ & $q = 1-p$ 
\\
 \hline
 \textbf{Support} &  $k=\{0,1\}$\\
 \hline
 \textbf{Probability Mass Function} & $p$ $\hspace{1.1cm}$ if $k=1$ \\ & $q=1-p$ if $k=0$ \\
 \hline
 \textbf{Cumulative Distribution Function} & 0 if $k < 0$ \\ & 1-p if $0\leq k < 1$ \\ & 1 if $k \geq 1$ \\
 \hline
 \textbf{Mean} & $p$ \\ 
 \hline
 \textbf{Variance} & $pq$\\
 \hline
 \textbf{Median} & $0$ if $p < 1/2$ \\ & [0,1] if $p=1/2$ \\ & 1 if $p > 1/2$ \\ 
 \hline
 \textbf{Mode} & $0$ if $p < 1/2$ \\ & [0,1] if $p=1/2$ \\ & 1 if $p > 1/2$ \\
 \hline
 \textbf{Moment Generating Function} & $q + pe^m$\\
 \hline
 \textbf{Skewness} & $\frac{q-p}{\sqrt{pq}}$\\
 \hline
 \textbf{Excess Kurtosis} &  $\frac{1-6pq}{pq}$\\
 \hline
       
  \end{tabular}
  
\textbf{To find Skewness}
\[ \mu_3= \frac{E(X^3) - 3 \mu \sigma^2 - \mu^3}{ \sigma^3} \]

\[\mu_3 = \frac{p - 3p \cdot pq - p^3}{pq \sqrt{pq}}\]

\[\mu_3 = \frac{p - 3p^2 (1 - p) - p^3}{pq \sqrt{pq}}\]

\[\mu_3 = \frac{p(1 - p^2) - 3p^2 (1 - p)}{pq \sqrt{pq}}\] 

\[\mu_3 = \frac{p(1 - p)(1+p) - 3p \cdot p (1 - p)}{pq \sqrt{pq}}\] 

\[\mu_3 = \frac{p (1 - p)(1+p - 3p)}{pq \sqrt{pq}}\] 

\[\mu_3 = \frac{p q(1 - 2p)}{pq \sqrt{pq}}\] 

\[\mu_3 = \frac{(q - p)}{\sqrt{pq}}\]


\textbf{To find Kurtosis}
\[ \mu_4 = \frac{ E(X^4) - 4 \mu E(X^3) + 6 \mu^2 E(X^2) - 3 \mu^4}{ \sigma^4} \]

\[\mu_4 = \frac{p - 4p \cdot p + 6p^2 \cdot p - 3 p^4}{(pq)^2}\]

\[\mu_4 = \frac{p(1-4p+6p^2 - 3p^3)}{p^2q^2}\]

\[\mu_4 = \frac{(1-4p+6p^2 - 3p^3)}{pq^2}\]

\[\mu_4 = \frac{(1-p)(3p^2 - 3p +1)}{pq^2}\]

\[\mu_4 = \frac{(3p^2 - 3p +1)}{pq}\]

\[\text{Excess Kurtosis } = \frac{(3p^2 - 3p +1)}{pq} - 3\]

\[\text{Excess Kurtosis } = \frac{3p^2 - 3p +1 - 3 pq}{pq}\]

\[\text{Excess Kurtosis } = \frac{3p(1- q) - 3p +1 - 3 pq}{pq}\]

\[\text{Excess Kurtosis } = \frac{3p- 3pq - 3p +1 - 3 pq}{pq}\]

\[\text{Excess Kurtosis } = \frac{1 - 6 pq}{pq}\]


\section{Binomial Distribution}

\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }& $n \in \{0,1,2, \dots \}$ \\ & $p \in [0,1]$ \\ & $q = 1-p$  \\
 \hline
 \textbf{Support} & $k \in \{ 0,1 \dots n \} $ \\
 \hline
 \textbf{Probability Mass Function} & 
 $ (^n_x)p^x(1-p)^{n-x}$
 \\
 \hline
 \textbf{Cumulative Distribution Function} &
 $\sum^k_{i=1}(^n_i)p^i(1-p)^{n-i}$ \\
 \hline
 \textbf{Mean} & $np$ \\ 
 \hline
 \textbf{Variance} & $np(1-p)$ \\
 \hline
 \textbf{Median} & $\lfloor np \rfloor $ \\ 
 \hline
 \textbf{Mode} & $\lfloor (n+1)p \rfloor $ \\
 \hline
 \textbf{Moment Generating Function} & $\sum^m_{k=0} \{ ^m_k\} n^{\underline{k}} p^k$ \\
 \hline
 \textbf{Skewness} & $\frac{q-p}{\sqrt{npq}}$ \\
 \hline
 \textbf{Excess Kurtosis} & $\frac{1-6pq}{npq}$  \\
 \hline
       
  \end{tabular}
  
  
The binomial distribution is a discrete probability distribution. Such a distribution consists of n Bernoulli trials(independent trials which have only 2 possible outputs and every trial has the same probability of success). 
\\ If X is a random variable which follows the Binomial distribution, it can be expressed as the following
\\ $X \sim Bin(n,p)$ where $n:$ no of trials and $p:$ probability of success. 
\[Pr(X=x) = (^n_x)p^x(1-p)^{n-x}\]
The shape of the graph depends on the values of n and p.


\section{Exponential Distribution}
\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }& $\lambda > 0$ \\
 \hline
 \textbf{Support} &  $x \in [0, \infty ) $ \\
 \hline
 \textbf{Probability Mass Function} & $\lambda e^{-\lambda x} $ \\
 \hline
 \textbf{Cumulative Distribution Function} &  $1 -  e^{-\lambda x}$\\
 \hline
 \textbf{Mean} & $1/\lambda $\\ 
 \hline
 \textbf{Variance} & $1/\lambda^2 $ \\
 \hline
 \textbf{Median} & $\frac{ln 2}{\lambda}$ \\ 
 \hline
 \textbf{Mode} & $0$\\
 \hline
 \textbf{Moment Generating Function} & $\frac{\lambda}{\lambda - t}$ for $t < \lambda $\\
 \hline
 \textbf{Skewness} & $2$ \\
 \hline
 \textbf{Excess Kurtosis} & $6$  \\
 \hline
       
  \end{tabular}
  
\textbf{To find Skewness}
\[ \mu_3= \frac{E(X^3) - 3 \mu \sigma^2 - \mu^3}{ \sigma^3} \]

\[\mu_3 = \frac{\frac{3!}{\lambda^3} - 3 \frac{1}{\lambda}\frac{1}{\lambda^2} - \frac{1}{\lambda^3}}{\frac{1}{\lambda^3}}\]

\[\mu_3 = 3! - 3- 1\]

\[\mu_3 = 2\]


\textbf{To find Kurtosis}
\[ \mu_4 = \frac{ E(X^4) - 4 \mu E(X^3) + 6 \mu^2 E(X^2) - 3 \mu^4}{ \sigma^4} \]
 
\[\mu_4 = \frac{\frac{4!}{\lambda^4} - 4 \frac{1}{\lambda}\frac{3!}{\lambda^3} +6 \frac{1}{\lambda^2}\frac{2}{\lambda^2} - 3 \frac{1}{\lambda^4}}{\frac{1}{\lambda^4}}\]
  
\[\mu_4 = 24 - 24 +12 -3\]
\[\mu_4 = 9\]
\[\text{Excess Kurtosis = } 6\]
\section{Gamma Distribution}


\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }&  $k > 0$ (shape) \\ & $\theta > 0$ (scale)\\
 \hline
 \textbf{Support} &  $x \in (0, \infty) $\\
 \hline
 \textbf{Probability Mass Function} &  $\frac{x^{k-1}e^{-x/\theta}}{\theta^k \Gamma(k)}$ \\
 \hline
 \textbf{Cumulative Distribution Function} & $\frac{1}{\Gamma(k)} \gamma(k, \frac{x}{\theta} )$ \\
 \hline
 \textbf{Mean} & $k \theta$ \\ 
 \hline
 \textbf{Variance} & $k \theta^2$ \\
 \hline
 \textbf{Median} &  No simple closed form \\ 
 \hline
 \textbf{Mode} & $(k-1)\theta$ for $k \geq 1$ \\ & $0$ for $k < 1$ \\
 \hline
 \textbf{Moment Generating Function} & $(1 - \theta t)^{-k} $ for $t < \frac{1}{\theta}$\\
 \hline
 \textbf{Skewness} & $\frac{2}{\sqrt{k}}$ \\
 \hline
 \textbf{Excess Kurtosis} & $\frac{6}{k}$  \\
 \hline
       
  \end{tabular}
  
  
\section{Logarithmic Distributions}


\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }&  $ 0 < p < 1$\\
 \hline
 \textbf{Support} & $k \in \{ 1, 2, 3 \dots \}$ \\
 \hline
 \textbf{Probability Mass Function} &  $\frac{-1}{\text{ln}(1-p)} \frac{p^k}{k}$ \\
 \hline
 \textbf{Cumulative Distribution Function} & $ 1 + \frac{B(p ;k+1,0)}{ln(1-p)}$ \\
 \hline
 \textbf{Mean} & $\frac{-1}{\text{ln}(1-p)} \frac{p}{1-p}$ \\ 
 \hline
 \textbf{Variance} & $- \frac{p^2 + p \text{ln}(1-p)}{(1-p)^2 (\text{ln}(1-p))^2}$ \\
 \hline
 \textbf{Median} &  \\ 
 \hline
 \textbf{Mode} & 1 \\
 \hline
 \textbf{Moment Generating Function} & $\frac{ln(1 - pe^t)}{ln (1-p)}$ for $t < - \text{ln } p $\\
 \hline
 \textbf{Skewness} & \\
 \hline
 \textbf{Excess Kurtosis} &   \\
 \hline
 
       
  \end{tabular}
  
Skewness and kurtosis dont have a closed form as this is a family of distributions
  
  
\section{Levy Distribution}


\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }& $\mu$ location \\ & $c > 0$ scale \\
 \hline
 \textbf{Support} & $ x \in [ \mu, \infty )$ \\
 \hline
 \textbf{Probability Mass Function} & $\sqrt{\frac{C}{2\pi}} \frac{e^{- \frac{e}{2(x - \mu)} }}{(x-\mu)^{3/2}}$ \\
 \hline
 \textbf{Cumulative Distribution Function} &  erfc $\Bigg( \sqrt{\frac{c}{2(x - \mu)}} \Bigg)$\\
 \hline
 \textbf{Mean} & $\infty$ \\ 
 \hline
 \textbf{Variance} & $\infty$ \\
 \hline
 \textbf{Median} & $\mu + c/2(erfc^{-1} (1/2))^2$ \\ 
 \hline
 \textbf{Mode} & $\mu + \frac{c}{3}$ \\
 \hline
 \textbf{Moment Generating Function} & undefined \\
 \hline
 \textbf{Skewness} & undefined\\
 \hline
 \textbf{Excess Kurtosis} & undefined \\
 \hline
       
  \end{tabular}
  
  
  
\section{Cauchy Distribution}


\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }& $x_0 \in \R$ (location) \\ & $\gamma > 0 $ and $\gamma \in \R$ (scale) \\
 \hline
 \textbf{Support} & $x \in (- \infty , + \infty)$ \\
 \hline
 \textbf{Probability Mass Function} &  $\frac{1}{\pi \gamma [1 + (\frac{x-x_0}{\gamma})^2 ]}$\\
 \hline
 \textbf{Cumulative Distribution Function} &  $\frac{1}{\pi}$ arctan $\Big( \frac{x - x_0}{\gamma} + \frac{1}{2}$\\
 \hline
 \textbf{Mean} & Undefined \\ 
 \hline
 \textbf{Variance} & Undefined \\
 \hline
 \textbf{Median} & $x_0$ \\ 
 \hline
 \textbf{Mode} & $x_0$ \\
 \hline
 \textbf{Moment Generating Function} & does not exist\\
 \hline
 \textbf{Skewness} & undefined\\
 \hline
 \textbf{Excess Kurtosis} & undefined \\
 \hline
       
  \end{tabular}
  
  
  
\section{Zipfs Law}
\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }& $s \geq 0$ (Real) \\ & $N \in \{ 1,2,3,4 \dots \}$ (Integers) \\
 \hline
 \textbf{Support} & $k \in \{ 1,2 \dots N \} $ \\
 \hline
 \textbf{Probability Mass Function} & 
 $ \frac{1}{k^s H_{N,s}}$
 \\
 \hline
 \textbf{Cumulative Distribution Function} &  $\frac{H_{k,s}}{H_{N,s}}$ \\
 \hline
 \textbf{Mean} & $\frac{H_{N,s-1}}{H_{N,s}}$ \\ 
 \hline
 \textbf{Variance} & $\frac{H_{N,s-2}}{H_{N,s}} - \frac{H^2_{N,s-1}}{H^2_{N,s}}$ \\
 \hline
 \textbf{Mode} & $1$ \\
 \hline
 \textbf{Moment Generating Function} & $\frac{1}{H_{N,s}} \sum^N_{n=1}\frac{e^{nm}}{n^s}$ \\
 \hline
 \textbf{Skewness} &  \\
 \hline
 \textbf{Excess Kurtosis} &  \\
 \hline
       
  \end{tabular}
  
  
Zipf's law is an empirical law formulated using mathematical statistics. It states that for many types of data studied in the physical and social sciences, the rank-frequency distribution is an inverse relation.

Zipf's law then predicts that out of a population of N elements, the normalized frequency of the element of rank k, f(k;s,N), where s is  the value of the exponent characterizing the distribution

\[ f(k:s,N) = \frac{1/k^s}{\sum_{n=1}^N (1/n^s) } = \frac{1}{k^s H_{N,s}}\] 

where $H_{N,s}$ is the Nth generalised Harmonic number.
\\ The limit as $N \to \infty$ is finite if $m > 1$, with the generalized harmonic number bounded by and converging to the Riemann zeta function.
\\ $\big($ The Reimann zeta function is generally denoted by $\zeta (s)$. It is a function of a complex variable defined as 
\[ \zeta(s) = \sum^\infty_{n=1} \frac{1}{n^s} \big) \] 

As long as the exponent s exceeds 1, it is possible for such a law to hold with infinitely many words, since if $s>1$,
\[ \zeta(s) = \sum^\infty_{n=1} \frac{1}{n^s} < \infty \]

As the exponent, $s$ increases, given a rank, it can be seen below that the normalised frequency of the element decreases. 
\\ \includegraphics{Zipfs Distribution graph.png}
\\ $***$ Zipf's law holds if the number of elements with a given frequency is a random variable with power law distribution, $p(x) = \alpha x^{-1 -1/s}$


\section{Weibull Distribution}

\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }& $\lambda \in (0,+\infty)$ (Scale) \\ & $k \in +\infty)$ (Shape) \\
 \hline
 \textbf{Support} & $x\in [0,+\infty)$ \\
 \hline
 \textbf{Probability Mass Function} & 
 $ \frac{1}{k^s H_{N,s}}$
 \\
 \hline
 \textbf{Cumulative Distribution Function} &  $\frac{H_{k,s}}{H_{N,s}}$ \\
 \hline
 \textbf{Mean} & $\frac{H_{N,s-1}}{H_{N,s}}$ \\ 
 \hline
 \textbf{Variance} & $\frac{H_{N,s-2}}{H_{N,s}} - \frac{H^2_{N,s-1}}{H^2_{N,s}}$ \\
 \hline
 \textbf{Mode} & $1$ \\
 \hline
 \textbf{Moment Generating Function} & $\frac{1}{H_{N,s}} \sum^N_{n=1}\frac{e^{nm}}{n^s}$ \\
 \hline
 \textbf{Skewness} & \\
 \hline
 \textbf{Excess Kurtosis} &  \\
 \hline
       
  \end{tabular}
  
The Weibull Distribution is a continuous probability distribution

\section{Student T-Distribution}

\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }&  \\
 \hline
 \textbf{Support} &  \\
 \hline
 \textbf{Probability Mass Function} &  \\
 \hline
 \textbf{Cumulative Distribution Function} &   \\
 \hline
 \textbf{Mean} & $0$ if $\nu > 1$ \\ & Undefined otherwise\\ 
 \hline
 \textbf{Variance} & $\frac{\nu}{\nu - 2}$ if $\nu > 2$ \\ & $\infty$ if $1 < \nu \leq 2$ \\ & Undefined otherwise \\
 \hline
 \textbf{Median} &  \\ 
 \hline
 \textbf{Mode} & \\
 \hline
 \textbf{Moment Generating Function} & does not exist \\
 \hline
 \textbf{Skewness} & $0 $ for $ \nu > 3 $ \\ & undefined otherwise \\
 \hline
 \textbf{Excess Kurtosis} & \textbf{TO BE UNDERSTOOD} \\
 \hline
       
  \end{tabular}

\textbf{To find Skewness}
\[ \mu_3= \frac{E(X^3) - 3 \mu \sigma^2 - \mu^3}{ \sigma^3} \]
\\ $E(X^3) = \mu = 0$ when $\nu > 3$
\\ $E(X^3)$ does not exist when $\nu < 3$

\[ \mu_3= 0 \text{ for } \nu > 3 \text{ ,undefined o/w}\]
\textbf{To find Kurtosis}
\[ \mu_4 = \frac{ E(X^4) - 4 \mu E(X^3) + 6 \mu^2 E(X^2) - 3 \mu^4}{ \sigma^4} \]
\\ $E(X^4)$ does not exist when $\nu < 4$



\section{Pareto Distribution}

\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }& $x_m > 0$ (scale) \\ & $\alpha >0$ (shape) \\
 \hline
 \textbf{Support} & $x \in [x_m, \infty)$ \\
 \hline
 \textbf{Probability Mass Function} & $\frac{\alpha x_m^\alpha}{x^{\alpha +1}}$ if $x \geq x_m$ \\ & $0$ if $x < x_m$ \\
 \hline
 \textbf{Cumulative Distribution Function} &  $1 - (\frac{x_m}{x})^\alpha$ \\
 \hline
 \textbf{Mean} & $\infty$ if $\alpha \leq 1$ \\ & $\frac{\alpha x_m}{\alpha - 1}$ if $\alpha > 1$ \\ 
 \hline
 \textbf{Variance} & $\infty$ if $\alpha \in (1,2] $ \\ & $\big( \frac{x_m}{\alpha - 1}\big) ^2 \frac{\alpha}{\alpha - 2}$ if $\alpha > 2$\\
 \hline
 \textbf{Median} & $x_m  \sqrt[\alpha]{2}$ \\ 
 \hline
 \textbf{Mode} & $x_m$\\
 \hline
 \textbf{Moment Generating Function} & does not exist \\
 \hline
 \textbf{Skewness} & $\frac{2(1+\alpha)}{(\alpha - 3)} \sqrt{\frac{\alpha - 2}{\alpha}}$\\
 \hline
 \textbf{Excess Kurtosis} & $\frac{6(\alpha^3 +\alpha^2 - 6\alpha -2 )}{\alpha(\alpha - 3)(\alpha - 4)}$ \\
 \hline
       
  \end{tabular}

\\ \underline{Mean}
\[ E(X) = \int^\infty _{x_{min}} x f(x) dx\]
\[ E(X) = \int^\infty _{x_{min}} x \frac{\alpha x_{min}^\alpha}{x^{\alpha+1}} dx\]
\[ E(X) = \alpha x_{min}^\alpha \int^\infty _{x_{min}} x^{1 - \alpha - 1} dx\]
\[ E(X) = \alpha x_{min}^\alpha \Big[ \frac{x^{- \alpha +1}}{-\alpha + 1} \Big]^\infty_{x_{min}} dx\]
\[ E(X) =  \frac{\alpha x_{min}^\alpha x_{min}^{-\alpha + 1}}{\alpha - 1} \]
\[ E(X) = \frac{\alpha x_{min}}{\alpha - 1} \]
If $\alpha > 1$, $E(X) = \frac{\alpha x_{min}}{\alpha - 1} $ 
\\ If $\alpha \leq 1$, $E(X) = \infty$
\underline{Variance}
\\ $Var(x) = E(X^2) - (E(X))^2$
\[ E(X^2)  = \int^\infty_{x_{min}} x^2 f(x) dx\]
\[ = \int^\infty_{x_{min}} x^2 \frac{\alpha x_{min}^\alpha}{x^{\alpha+1}} dx\]
\[ = \alpha x_{min}^\alpha \int^\infty_{x_{min}} x^{2-\alpha-1} \]
\[ = \alpha x_{min}^\alpha \int^\infty_{x_{min}} x^{1-\alpha} \]
\[ = \alpha x_{min}^\alpha \Big[ \frac{x^{1-\alpha+1}}{1-\alpha+1} \Big]^\infty_{x_{min}} \]
\[ = \alpha x_{min}^\alpha \frac{x_{min}^{2-\alpha}}{\alpha - 2}  \]
\[ = x_{min}^2 \frac{\alpha}{\alpha-2}  \text{ when } \alpha > 2 \]
\[ Var(x) = x_{min}^2 \frac{\alpha}{\alpha-2} - \Big( \frac{\alpha x_{min}}{\alpha - 1} \Big)^2 \]
\[ = x^2_{min} \Big( \frac{\alpha}{\alpha-2} - \frac{\alpha^2}{(\alpha - 1)^2} \Big) \]
\[ = x^2_{min} \Big( \frac{\alpha(\alpha - 1)^2 - \alpha^2(\alpha-2)}{(\alpha - 2)(\alpha - 1)^2} \Big) \]
\[ = x^2_{min} \alpha \Big( \frac{(\alpha - 1)^2 - \alpha(\alpha-2)}{(\alpha - 2)(\alpha - 1)^2} \Big)\]
\[ = x^2_{min} \alpha \Big( \frac{(\alpha^2 + 1 - 2 \alpha - \alpha^2 + 2 \alpha )}{(\alpha - 2)(\alpha - 1)^2} \Big)\]
\[ = x^2_{min} \alpha \Big( \frac{1}{(\alpha - 2)(\alpha - 1)^2} \Big)\]
\[ Var(X) = \frac{x^2_{min} \alpha }{(\alpha - 2)(\alpha - 1)^2} \text{ when } \alpha > 2\]
\[ Var(x) = \infty \text{ when } \alpha \in (1,2] \]
\\
\\ \underline{Higher Moments}
\[ E (X^m) = \int^\infty_{x_{min}} x^m  f(x) dx\]
\[ E (X^m) = \int^\infty_{x_{min}} x^m \frac{\alpha x_{min}^\alpha}{x^{\alpha+1}} dx\]
\[ E (X^m) = \alpha x_{min}^\alpha \int^\infty_{x_{min}} x^{m - \alpha -1} dx\]
\[ E (X^m) = \alpha x_{min}^\alpha \Big[ \frac{x^{m - \alpha -1+1}}{m - \alpha -1+1} \Big]^\infty_{x_{min}} \]
\[ E (X^m) = \alpha x_{min}^\alpha \Big[ \frac{x^{m - \alpha}}{m - \alpha} \Big]^\infty_{x_{min}} \]
\[ E (X^m) = \alpha x_{min}^\alpha \Big[ \frac{x_{min}^{m - \alpha}}{\alpha - m} \Big] \]
\[ E (X^m) = x_{min}^m \frac{\alpha}{\alpha - m}  \]
\\
\\ If $n< \alpha$, $E (X^m) = x_{min}^m \frac{\alpha}{\alpha - m}$.
\\ If $n \geq \alpha $, $E(X^m) = \infty$
\\
\\ .


\textbf{To find Skewness}
\[ \mu_3= \frac{E(X^3) - 3 \mu \sigma^2 - \mu^3}{ \sigma^3} \]

\[\mu_3 = \frac{\frac{\alpha x_m^3}{\alpha - 3} - 3 (\frac{\alpha x_m}{\alpha-1}) \frac{x_m^2 \alpha}{(\alpha - 1)^2 (\alpha -2)} - (\frac{\alpha x_m}{\alpha - 1})^3}{\frac{x_m^2 \alpha}{(\alpha - 1)^2 (\alpha - 2)}\frac{x_m}{(\alpha - 1)} \sqrt{\frac{\alpha}{\alpha - 2}}}\]

\[\mu_3 = \frac{\frac{\alpha x_m^3}{\alpha - 3} - 3 \frac{\alpha^2 x_m^3}{(\alpha-1)^3(\alpha -2)} - \frac{\alpha^3 x_m^3}{(\alpha - 1)^3}}{\frac{x_m^3 \alpha}{(\alpha - 1)^3(\alpha - 2)} \sqrt{\frac{\alpha}{\alpha - 2}}}\]

\[\mu_3 = \frac{(\alpha - 1)^3(\alpha - 2) - 3 \alpha (\alpha - 3) - \alpha^2 (\alpha - 2)(\alpha - 3)}{(\alpha - 3) \sqrt{\frac{\alpha}{\alpha - 2}}}\]

\[\mu_3 = \frac{\alpha^4 - 3 \alpha^3 +3 \alpha^2 - \alpha - 2\alpha^3 + 6 \alpha^2 - 6\alpha + 2 - 3 \alpha^2 + 9 \alpha - \alpha^4 + 2 \alpha^3 + 3 \alpha^3 - 6 \alpha^2 }{(\alpha - 3) \sqrt{\frac{\alpha}{\alpha - 2}}}\]

\[\mu_3 = \frac{2(\alpha +1)}{\alpha-3} \sqrt{\frac{\alpha - 2}{\alpha}}\]


\textbf{To find Kurtosis}
\[ \mu_4 = \frac{ E(X^4) - 4 \mu E(X^3) + 6 \mu^2 E(X^2) - 3 \mu^4}{ \sigma^4} \]

\[\mu_4 = \frac{\frac{\alpha x_m^4}{\alpha - 4} - \frac{4 \alpha x_m}{\alpha - 1} \frac{\alpha x_m^3}{(\alpha - 3)} + 6 (\frac{\alpha x_m}{\alpha - 1})^2 \frac{\alpha x_m^2}{(\alpha - 2)} - 3(\frac{\alpha x_m}{\alpha - 1})^4 }{\frac{x_m^4 \alpha^2}{(\alpha - 1)^4 (\alpha -2)^2}}\]

\[\mu_4 = \frac{\frac{\alpha x_m^4}{\alpha - 4} - \frac{4 \alpha^2 x_m^4}{(\alpha - 1)(\alpha - 3)} + 6 \frac{\alpha^3 x_m^4}{(\alpha - 1)^2 (\alpha - 2)} - 3\frac{\alpha^4 x_m^4}{(\alpha - 1)^4} }{\frac{x_m^4 \alpha^2}{(\alpha - 1)^4 (\alpha -2)^2}}\]

\[\mu_4 = \frac{(\alpha -1)^4 (\alpha - 2)(\alpha - 3) - 4 \alpha(\alpha - 1)^3(\alpha - 2)(\alpha - 4) + 6 \alpha^2 (\alpha - 1)^2(\alpha - 3)(\alpha - 4) - 3 \alpha^3(\alpha - 2)(\alpha - 3)(\alpha - 4)}{\frac{\alpha (\alpha - 3)(\alpha - 4)}{(\alpha - 2)}}\]

\[\mu_4 = \frac{(9 \alpha^2 + 3 \alpha + 6)(\alpha - 2)}{\alpha (\alpha - 3)(\alpha - 4)} \text{when $\alpha > 4$}\]

\textbf{Excess Kurtosis} 
\[\text{Excess kurtosis }= \frac{(9 \alpha^2 + 3 \alpha + 6)(\alpha - 2)}{\alpha (\alpha - 3)(\alpha - 4)} - 3\]

\[\text{Excess kurtosis }= \frac{9 \alpha^3 - 15 \alpha^2 - 12 - 3 \alpha (\alpha - 3)(\alpha - 4) }{\alpha (\alpha - 3)(\alpha - 4)}\]

\[\text{Excess kurtosis }= \frac{9 \alpha^3 - 15 \alpha^2 - 12 - 3 \alpha^3 + 21 \alpha^2 - 36 \alpha}{\alpha (\alpha - 3)(\alpha - 4)}\]

\[\text{Excess kurtosis }= \frac{6(\alpha^3 + \alpha^2 - 6 \alpha - 2) }{\alpha (\alpha - 3)(\alpha - 4)} \text{when $\alpha > 4$}\]
\\
\\
\newpage
\underline{\textbf{NOTES}}
\\\textbf{Power Law}
\\$ f(x) = k x^{-\alpha}$
\\
\\ \underline{Finding the value of k}
\[\int^\infty_{x_{min}} f(x) dx = 1 \]
\[ \int^\infty_{x_{min}} kx^{-\alpha} dx = 1\]
\[ k \int^\infty_{x_{min}} x^{-\alpha} dx = 1\]
\[ k \Big[ \frac{x^{-\alpha +1}}{-\alpha + 1} \Big]^\infty_{x_{min}} = 1\]
\[ k \frac{x_{min}^{-\alpha +1}}{\alpha - 1} = 1\]
\[ k = \frac{\alpha - 1}{x_{min}^{1 -\alpha}}\]
\\
\\
\\ \underline{Mean} 
\\ A well defined mean exists $ \in [x_{min}, \infty )$ iff $\alpha \geq 2$ 
\[ E(X) = \int^\infty_{x_{min}} x f(x) dx \]
\[ = k \int^\infty_{x_{min}} x \frac{1}{x^\alpha} dx\]
\[ = k \int^\infty_{x_{min}} x^{1-\alpha} dx\]
\[ = \frac{\alpha - 1}{x_{min}^{1 -\alpha}} \Big[ \frac{x^{2 -\alpha}}{2 - \alpha } \Big]^\infty_{x_{min}} \]
\[ = \frac{\alpha - 1}{x_{min}^{1 -\alpha}} \Big[ \frac{x_{min}^{2 -\alpha}}{\alpha -2 } \Big] \]
\[ = \frac{\alpha - 1}{\alpha - 2} x_{min}\]
\\
\\ If $\alpha = 2$ \[ E(X) = k \int^\infty_{x_{min}} \frac{1}{x} dx \]
$log(\infty)$ does not exist, $\therefore$ $\alpha \neq 2$
\\
\\ If $\alpha < 2$, the integral does not converge and hence does not exist.
\newpage
\underline{Variance} 
\\ $Var(x) = E(X^2) - (E(X))^2$
\\ \[ E(X)^2  = \int^\infty_{x_{min}} x^2 f(x) dx\]
\[  =  \int^\infty_{x_{min}} x^2 \frac{\alpha - 1 }{x_{min}^{1-\alpha}}x^{-\alpha} dx\]
\[  =  \frac{\alpha - 1 }{x_{min}^{1-\alpha}} \int^\infty_{x_{min}} x^{2 -\alpha} dx\]
\[  =  \frac{\alpha - 1 }{x_{min}^{1-\alpha}} \Big[ \frac{x^{2 -\alpha+1}}{2- \alpha +1} \Big]^\infty_{x_{min}} \]
\[  =  \frac{\alpha - 1 }{x_{min}^{1-\alpha}} \Big[ \frac{x^{3 -\alpha}}{3 - \alpha } \Big]^\infty_{x_{min}} \]
\[  =  \frac{\alpha - 1 }{x_{min}^{1-\alpha}} \Big[ \frac{x_{min}^{3 -\alpha}}{\alpha - 3 } \Big] \]
\[  =  \frac{\alpha - 1 }{\alpha - 3} x_{min}^{3 -\alpha - 1+ \alpha}\]
\[ = \frac{\alpha - 1 }{\alpha - 3} x_{min}^2\]
\\
\[ Var(X) = E(X^2) - (E(X))^2\]
\[ Var(X) = \frac{\alpha - 1 }{\alpha - 3} x_{min}^2 - (\frac{\alpha - 1}{\alpha - 2} x_{min})^2\]
\[ Var(X) = x_{min}^2 \Big( \frac{\alpha -1}{\alpha - 3} - (\frac{\alpha - 1}{\alpha - 2}) ^2\Big)\]
\\ 
\\ 
\\ \underline{Higher Moments}
\[ E(X^m) = \int^\infty_{x_{min}} x^m f(x) dx \] 
\[ E(X^m) = k \int^\infty_{x_{min}} x^m x^{-\alpha} dx\]
\[ E(X^m) = \frac{\alpha - 1}{x_{min}^{1 -\alpha}} \int^\infty_{x_{min}} x^{m-\alpha} dx\]
\[ E(X^m) = \frac{\alpha - 1}{x_{min}^{1 -\alpha}} \Big[ \frac{x^{m-\alpha + 1}}{m- \alpha +1} \Big]^\infty _{x_{min}} \]
\[ E(X^m) = \frac{\alpha - 1}{x_{min}^{1 -\alpha}} \Big[ \frac{x_{min}^{-\alpha + 1} x_{min}^m}{\alpha -m  -1} \Big] \]
\[ E(X^m) = \frac{\alpha - 1}{\alpha - m - 1} x_{min}^m\]
