\documentclass[14pt, a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\theoremstyle{definition}
\newtheorem*{definition}{Definition}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\urlstyle{same}

\title{Analysis Of Distributions \\ By Anjali Susan Oommen}
\date{}

\begin{document}

\maketitle

This analysis of distributions was written as part of my honours project, "What are statistical fat tails and their consequences?". According to Nassim Nicholas Taleb, distributions can be divided into $3$ categories on the basis of their tails : Fat Tails, Semi Thick Tails and Thin Tails. (Taleb N.N ,2021)
\\ An important example of fat tailed distribution is the power law family. In my research done so far, I have found 5 characterising properties for fat tails.
\begin{enumerate}
\item A fat-tailed distribution is a probability distribution that exhibits a large skewness or kurtosis, relative to that of either a normal distribution or an exponential distribution. (Wikipedia)
\item In a fat-tailed distribution, a small number of observations in a given dataset will represent the bulk of the statistical properties. Remote events happen less often but they command much greater respect. (Taleb N.N ,2021)
\item A distribution is said to be fat tailed if the ratio of standard deviation to mean absolute deviation is greater than 1.5. (Taleb N.N ,2021)
\item Some authors reserve the term “fat tail” to mean the subclass of heavy tailed distributions that exhibit power law decay behavior as well as infinite variance. Heavy tailed distributions are those that have heavier tails than the exponential distribution. (Glen S)
\item Fat tailed distributions either don’t follow the Law of Large Numbers or follow it much slower than the Gaussian distribution does. (Gaussian taken as the standard). (Taleb N.N ,2021)
\end{enumerate}
Semi thick tails are those subexponential distributions which are not power laws. A subexponential distribution is one whose tail decreases slower than any exponential tail.An example of such a distribution is $f(x) = e^{-x^2}$.  It is shown in Fig 1. All subexponential distributions can be found to lie below the exponential distribution, $f(x) = e^x$ on the graph preasymptotically.


\begin{center}
    \includegraphics[width=80mm]{Exponential vs Subexponential Distributions.jpg}
    \\ Fig 1
\end{center}

The thin tailed distributions are the Normal Distribution, Bernoulli Distribution and Binomial Distribution. They all have almost 0 probability in their tails.

\newpage
In order to better understand what these categories mean, I have put together a few important distributions and looked at their properties (such as moments which I derived in a few cases). The focus of the project will be fat tailed distributions so a more detailed study has been done on the Pareto Distribution which is one the most widely used fat tailed distributions in today's world. All definitions which I think are pertinent to this paper can be found in $ ________ $. Taleb's categorization of the distributions can be found in Table 1. 

\includegraphics{Taleb Distributions.jpg}
\\ \centerline{\textbf{Table 1}}

 In sections 1 - 11 , the properties of these distributions as well as a few other notable ones have been looked at. Section 12 of this paper includes the derivation of the moments of the Power Law Family of Distributions as these are a class of fat tailed distributions.
 
 
\section{Normal Distribution}

\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }& $\mu \in \R$ (mean) (location parameter) \\ & $\sigma^2 \in \R_{>0}$ (variance) \\
 \hline
 \textbf{Support} & $x \in \R$ \\
 \hline
 \textbf{Probability Mass Function} & $\frac{1}{\sigma \sqrt{2 \pi } } e^{ - \frac{1}{2} \big( \frac{x-\mu}{\sigma} \big) ^2 } $  \\
 \hline
 \textbf{Cumulative Distribution Function} & $\frac{1}{2} \Bigg[1 + erf(\frac{x - \mu}{\sigma \sqrt{2}}  ) \Bigg]$ \\
 \hline
 \textbf{Mean} & $\mu$ \\ 
 \hline
 \textbf{Variance} & $\sigma^2$ \\
 \hline
 \textbf{Median} & $\mu$ \\ 
 \hline
 \textbf{Mode} & $\mu$ \\
 \hline
 \textbf{Moment Generating Function} & exp$(\mu t + \frac{\mu^2 t^2}{2})$ \\
 \hline
 \textbf{Skewness} & $0$  \\
 \hline
 \textbf{Excess Kurtosis} & $3(\sigma^4 -1)$ \\
 \hline
       
  \end{tabular}
  
  
Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve. While normal distributions are symmetric, but not all symmetric distributions are normal. In a normal distribution, mean = median = mode. It is also unimodal in nature. It belongs to the stable family of distributions. (Wikipedia)

It is a type of continuous probability distribution for a real-valued random variable. The general form of its probability density function is 
\[ f(x) = \frac{1}{\sigma \sqrt{2 \pi } } e^{ - \frac{1}{2} \big( \frac{x-\mu}{\sigma} \big) ^2 }  \]
where $\mu$ is the mean of the distribution and $\sigma$ is the standard deviation.

For a normal distribution, $68$ percent of the observations are within $+/-$ one standard deviation of the mean, $95$ percent are within $+/-$ two standard deviations, and $99.7$ percent are within $+/-$ three standard deviations.


\subsection{Standard Normal Distribution}
In a standard normal distribution the mean is zero and the standard deviation is 1. It has zero skew and a kurtosis of 3. Every normal distribution can be standardised. A normal dist is a standard normal distribution stretched by a the standard deviation of the normal distribution and translated by the mean of the normal distribution. The general probability density function of a standard normal distribution is 
\[ f(x) = \frac{e^{-x^2 / 2}}{\sqrt{2 \pi} } \]

A normal distribution can be standardised by giving the observations in the dataset of the distribution a z-score 
\[ z = \frac{x - \mu}{\sigma}\]

\section{Bernoulli Distribution}


\begin{tabular}{|c|c|}  %no. of columns
\hline
\textbf{Parameters }&  $p \in [0,1]$ \\ & $q = 1-p$ 
\\
 \hline
 \textbf{Support} &  $k=\{0,1\}$\\
 \hline
 \textbf{Probability Mass Function} & $p$ $\hspace{1.1cm}$ if $k=1$ \\ & $q=1-p$ if $k=0$ \\
 \hline
 \textbf{Cumulative Distribution Function} & 0 if $k < 0$ \\ & 1-p if $0\leq k < 1$ \\ & 1 if $k \geq 1$ \\
 \hline
 \textbf{Mean} & $p$ \\ 
 \hline
 \textbf{Variance} & $pq$\\
 \hline
 \textbf{Median} & $0$ if $p < 1/2$ \\ & [0,1] if $p=1/2$ \\ & 1 if $p > 1/2$ \\ 
 \hline
 \textbf{Mode} & $0$ if $p < 1/2$ \\ & [0,1] if $p=1/2$ \\ & 1 if $p > 1/2$ \\
 \hline
 \textbf{Moment Generating Function} & $q + pe^m$\\
 \hline
 \textbf{Skewness} & $\frac{q-p}{\sqrt{pq}}$\\
 \hline
 \textbf{Excess Kurtosis} &  $\frac{1-6pq}{pq}$\\
 \hline
       
  \end{tabular}

The Bernoulli distribution (which was named after Swiss mathematician Jacob Bernoulli) is the discrete probability distribution of a random variable which takes only 2 values:  1 and 0.
\\ $P(1) = p$
\\ $P(0) = 1-p = q$


\textbf{To find Skewness}
\[ \mu_3= \frac{E(X^3) - 3 \mu \sigma^2 - \mu^3}{ \sigma^3} \]

\[\mu_3 = \frac{p - 3p \cdot pq - p^3}{pq \sqrt{pq}}\]

\[\mu_3 = \frac{p - 3p^2 (1 - p) - p^3}{pq \sqrt{pq}}\]

\[\mu_3 = \frac{p(1 - p^2) - 3p^2 (1 - p)}{pq \sqrt{pq}}\] 

\[\mu_3 = \frac{p(1 - p)(1+p) - 3p \cdot p (1 - p)}{pq \sqrt{pq}}\] 

\[\mu_3 = \frac{p (1 - p)(1+p - 3p)}{pq \sqrt{pq}}\] 

\[\mu_3 = \frac{p q(1 - 2p)}{pq \sqrt{pq}}\] 

\[\mu_3 = \frac{(q - p)}{\sqrt{pq}}\]


\textbf{To find Kurtosis}
\[ \mu_4 = \frac{ E(X^4) - 4 \mu E(X^3) + 6 \mu^2 E(X^2) - 3 \mu^4}{ \sigma^4} \]

\[\mu_4 = \frac{p - 4p \cdot p + 6p^2 \cdot p - 3 p^4}{(pq)^2}\]

\[\mu_4 = \frac{p(1-4p+6p^2 - 3p^3)}{p^2q^2}\]

\[\mu_4 = \frac{(1-4p+6p^2 - 3p^3)}{pq^2}\]

\[\mu_4 = \frac{(1-p)(3p^2 - 3p +1)}{pq^2}\]

\[\mu_4 = \frac{(3p^2 - 3p +1)}{pq}\]

\[\text{Excess Kurtosis } = \frac{(3p^2 - 3p +1)}{pq} - 3\]

\[\text{Excess Kurtosis } = \frac{3p^2 - 3p +1 - 3 pq}{pq}\]

\[\text{Excess Kurtosis } = \frac{3p(1- q) - 3p +1 - 3 pq}{pq}\]

\[\text{Excess Kurtosis } = \frac{3p- 3pq - 3p +1 - 3 pq}{pq}\]

\[\text{Excess Kurtosis } = \frac{1 - 6 pq}{pq}\]


\section{Binomial Distribution}

\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }& $n \in \{0,1,2, \dots \}$ \\ & $p \in [0,1]$ \\ & $q = 1-p$  \\
 \hline
 \textbf{Support} & $k \in \{ 0,1 \dots n \} $ \\
 \hline
 \textbf{Probability Mass Function} & 
 $ (^n_x)p^x(1-p)^{n-x}$
 \\
 \hline
 \textbf{Cumulative Distribution Function} &
 $\sum^k_{i=1}(^n_i)p^i(1-p)^{n-i}$ \\
 \hline
 \textbf{Mean} & $np$ \\ 
 \hline
 \textbf{Variance} & $np(1-p)$ \\
 \hline
 \textbf{Median} & $\lfloor np \rfloor $ \\ 
 \hline
 \textbf{Mode} & $\lfloor (n+1)p \rfloor $ \\
 \hline
 \textbf{Moment Generating Function} & $\sum^m_{k=0} \{ ^m_k\} n^{\underline{k}} p^k$ \\
 \hline
 \textbf{Skewness} & $\frac{q-p}{\sqrt{npq}}$ \\
 \hline
 \textbf{Excess Kurtosis} & $\frac{1-6pq}{npq}$  \\
 \hline
       
  \end{tabular}
  
  
The binomial distribution is a discrete probability distribution. Such a distribution consists of n Bernoulli trials(independent trials which have only 2 possible outputs and every trial has the same probability of success). 
\\ If X is a random variable which follows the Binomial distribution, it can be expressed as the following
\\ $X \sim Bin(n,p)$ where $n:$ no of trials and $p:$ probability of success. 
\[Pr(X=x) = (^n_x)p^x(1-p)^{n-x}\]
The shape of the graph depends on the values of n and p.


\section{Gamma Distribution}


\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }&  $k > 0$ (shape) \\ & $\theta > 0$ (scale)\\
 \hline
 \textbf{Support} &  $x \in (0, \infty) $\\
 \hline
 \textbf{Probability Mass Function} &  $\frac{x^{k-1}e^{-x/\theta}}{\theta^k \Gamma(k)}$ \\
 \hline
 \textbf{Cumulative Distribution Function} & $\frac{1}{\Gamma(k)} \gamma(k, \frac{x}{\theta} )$ \\
 \hline
 \textbf{Mean} & $k \theta$ \\ 
 \hline
 \textbf{Variance} & $k \theta^2$ \\
 \hline
 \textbf{Median} &  No simple closed form \\ 
 \hline
 \textbf{Mode} & $(k-1)\theta$ for $k \geq 1$ \\ & $0$ for $k < 1$ \\
 \hline
 \textbf{Moment Generating Function} & $(1 - \theta t)^{-k} $ for $t < \frac{1}{\theta}$\\
 \hline
 \textbf{Skewness} & $\frac{2}{\sqrt{k}}$ \\
 \hline
 \textbf{Excess Kurtosis} & $\frac{6}{k}$  \\
 \hline
       
  \end{tabular}
 
The gamma distribution is a two-parameter family of continuous probability distributions. The exponential distribution, Erlang distribution, and chi-square distribution are special cases of the gamma distribution. The exponential distribution is a special case of the gamma distribution where $k = 1$ and $\theta = \frac{1}{\lambda}$
  
\subsection{Exponential Distribution}
\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }& $\lambda > 0$ \\
 \hline
 \textbf{Support} &  $x \in [0, \infty ) $ \\
 \hline
 \textbf{Probability Mass Function} & $\lambda e^{-\lambda x} $ \\
 \hline
 \textbf{Cumulative Distribution Function} &  $1 -  e^{-\lambda x}$\\
 \hline
 \textbf{Mean} & $1/\lambda $\\ 
 \hline
 \textbf{Variance} & $1/\lambda^2 $ \\
 \hline
 \textbf{Median} & $\frac{ln 2}{\lambda}$ \\ 
 \hline
 \textbf{Mode} & $0$\\
 \hline
 \textbf{Moment Generating Function} & $\frac{\lambda}{\lambda - t}$ for $t < \lambda $\\
 \hline
 \textbf{Skewness} & $2$ \\
 \hline
 \textbf{Excess Kurtosis} & $6$  \\
 \hline
       
  \end{tabular}
  
The exponential distribution is the probability distribution of the time between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant rate.It is the continuous analogue of the geometric distribution. The distribution has the memorylessness property. 
\textbf{To find Skewness}
\[ \mu_3= \frac{E(X^3) - 3 \mu \sigma^2 - \mu^3}{ \sigma^3} \]

\[\mu_3 = \frac{\frac{3!}{\lambda^3} - 3 \frac{1}{\lambda}\frac{1}{\lambda^2} - \frac{1}{\lambda^3}}{\frac{1}{\lambda^3}}\]

\[\mu_3 = 3! - 3- 1\]

\[\mu_3 = 2\]


\textbf{To find Kurtosis}
\[ \mu_4 = \frac{ E(X^4) - 4 \mu E(X^3) + 6 \mu^2 E(X^2) - 3 \mu^4}{ \sigma^4} \]
 
\[\mu_4 = \frac{\frac{4!}{\lambda^4} - 4 \frac{1}{\lambda}\frac{3!}{\lambda^3} +6 \frac{1}{\lambda^2}\frac{2}{\lambda^2} - 3 \frac{1}{\lambda^4}}{\frac{1}{\lambda^4}}\]
  
\[\mu_4 = 24 - 24 +12 -3\]
\[\mu_4 = 9\]
\[\text{Excess Kurtosis = } 6\]
  
\section{Logarithmic Distributions}


\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }&  $ 0 < p < 1$\\
 \hline
 \textbf{Support} & $k \in \{ 1, 2, 3 \dots \}$ \\
 \hline
 \textbf{Probability Mass Function} &  $\frac{-1}{\text{ln}(1-p)} \frac{p^k}{k}$ \\
 \hline
 \textbf{Cumulative Distribution Function} & $ 1 + \frac{B(p ;k+1,0)}{ln(1-p)}$ \\
 \hline
 \textbf{Mean} & $\frac{-1}{\text{ln}(1-p)} \frac{p}{1-p}$ \\ 
 \hline
 \textbf{Variance} & $- \frac{p^2 + p \text{ln}(1-p)}{(1-p)^2 (\text{ln}(1-p))^2}$ \\
 \hline
 \textbf{Median} &  \\ 
 \hline
 \textbf{Mode} & 1 \\
 \hline
 \textbf{Moment Generating Function} & $\frac{ln(1 - pe^t)}{ln (1-p)}$ for $t < - \text{ln } p $\\
 \hline
 \textbf{Skewness} & \\
 \hline
 \textbf{Excess Kurtosis} &   \\
 \hline
 
       
  \end{tabular}
  
The logarithmic distribution (sometimes known as the Logarithmic Series distribution) is a discrete, positive distribution, peaking at x = 1, with one parameter and a long right tail. 
  
  
\section{Levy Distribution}


\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }& $\mu$ location \\ & $c > 0$ scale \\
 \hline
 \textbf{Support} & $ x \in [ \mu, \infty )$ \\
 \hline
 \textbf{Probability Mass Function} & $\sqrt{\frac{C}{2\pi}} \frac{e^{- \frac{e}{2(x - \mu)} }}{(x-\mu)^{3/2}}$ \\
 \hline
 \textbf{Cumulative Distribution Function} &  erfc $\Bigg( \sqrt{\frac{c}{2(x - \mu)}} \Bigg)$\\
 \hline
 \textbf{Mean} & $\infty$ \\ 
 \hline
 \textbf{Variance} & $\infty$ \\
 \hline
 \textbf{Median} & $\mu + c/2(erfc^{-1} (1/2))^2$ \\ 
 \hline
 \textbf{Mode} & $\mu + \frac{c}{3}$ \\
 \hline
 \textbf{Moment Generating Function} & undefined \\
 \hline
 \textbf{Skewness} & undefined\\
 \hline
 \textbf{Excess Kurtosis} & undefined \\
 \hline
       
  \end{tabular}
\\ The Lévy distribution,(which was named after Paul Lévy) is a continuous probability distribution for a non-negative random variable. [In spectroscopy, this distribution, with frequency as the dependent variable, is known as a van der Waals profile.] It belongs to the stable family of distributions. 
  
\section{Cauchy Distribution}


\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }& $x_0 \in \R$ (location) \\ & $\gamma > 0 $ and $\gamma \in \R$ (scale) \\
 \hline
 \textbf{Support} & $x \in (- \infty , + \infty)$ \\
 \hline
 \textbf{Probability Mass Function} &  $\frac{1}{\pi \gamma [1 + (\frac{x-x_0}{\gamma})^2 ]}$\\
 \hline
 \textbf{Cumulative Distribution Function} &  $\frac{1}{\pi}$ arctan $\Big( \frac{x - x_0}{\gamma} + \frac{1}{2}$\\
 \hline
 \textbf{Mean} & Undefined \\ 
 \hline
 \textbf{Variance} & Undefined \\
 \hline
 \textbf{Median} & $x_0$ \\ 
 \hline
 \textbf{Mode} & $x_0$ \\
 \hline
 \textbf{Moment Generating Function} & does not exist\\
 \hline
 \textbf{Skewness} & undefined\\
 \hline
 \textbf{Excess Kurtosis} & undefined \\
 \hline
       
  \end{tabular}
  
The Cauchy distribution (which is named after Augustin Cauchy) is a continuous probability distribution. It is also known, especially among physicists, as the Lorentz distribution. It belongs to the stable family of distributions.

The first known occurrence of the Cauchy distribution was in 1659 when a function with the form of the density function of the Cauchy distribution was studied geometrically by Fermat.It later was known as the witch of Agnesi, after Agnesi included it as an example in her calculus textbook published in 1784. Contrary to common belief, the first explicit analysis of the properties of the Cauchy distribution was published by the French mathematician Poisson in 1824, with Cauchy only becoming associated with it during an academic controversy regarding the importance of it's undefined mean and variance in 1853. (Cauchy Distribution, Wikipedia)
  
  
\section{Zipfs Law}
\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }& $s \geq 0$ (Real) \\ & $N \in \{ 1,2,3,4 \dots \}$ (Integers) \\
 \hline
 \textbf{Support} & $k \in \{ 1,2 \dots N \} $ \\
 \hline
 \textbf{Probability Mass Function} & 
 $ \frac{1}{k^s H_{N,s}}$
 \\
 \hline
 \textbf{Cumulative Distribution Function} &  $\frac{H_{k,s}}{H_{N,s}}$ \\
 \hline
 \textbf{Mean} & $\frac{H_{N,s-1}}{H_{N,s}}$ \\ 
 \hline
 \textbf{Variance} & $\frac{H_{N,s-2}}{H_{N,s}} - \frac{H^2_{N,s-1}}{H^2_{N,s}}$ \\
 \hline
 \textbf{Mode} & $1$ \\
 \hline
 \textbf{Moment Generating Function} & $\frac{1}{H_{N,s}} \sum^N_{n=1}\frac{e^{nm}}{n^s}$ \\
 \hline

  \end{tabular}
  
  
Zipf's law is an empirical law formulated using mathematical statistics. It states that for many types of data studied in the physical and social sciences, the rank-frequency distribution is an inverse relation.

Zipf's law then predicts that out of a population of N elements, the normalized frequency of the element of rank k, f(k;s,N), where s is  the value of the exponent characterizing the distribution

\[ f(k:s,N) = \frac{1/k^s}{\sum_{n=1}^N (1/n^s) } = \frac{1}{k^s H_{N,s}}\] 

where $H_{N,s}$ is the Nth generalised Harmonic number.
\\ The limit as $N \to \infty$ is finite if $m > 1$, with the generalized harmonic number bounded by and converging to the Riemann zeta function.
\\ $\big($ The Reimann zeta function is generally denoted by $\zeta (s)$. It is a function of a complex variable defined as 
\[ \zeta(s) = \sum^\infty_{n=1} \frac{1}{n^s} \big) \] 

As long as the exponent s exceeds 1, it is possible for such a law to hold with infinitely many words, since if $s>1$,
\[ \zeta(s) = \sum^\infty_{n=1} \frac{1}{n^s} < \infty \]

As the exponent, $s$ increases, given a rank, it can be seen below that the normalised frequency of the element decreases. 
\\ \includegraphics{Zipfs Distribution graph.png}
\\ $***$ Zipf's law holds if the number of elements with a given frequency is a random variable with power law distribution, $p(x) = \alpha x^{-1 -1/s}$
\\ $***$ The skewness and kurtosis are rarely talked about for this distribution as it comes from an empirical law.
\section{Weibull Distribution}

\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }& $\lambda \in (0,+\infty)$ (Scale) \\ & $k \in +\infty)$ (Shape) \\
 \hline
 \textbf{Support} & $x\in [0,+\infty)$ \\
 \hline
 \textbf{Probability Mass Function} & 
 $ \frac{k}{\lambda} (\frac{x}{\lambda})^{k-1} e^{-(x/\lambda)^k}$
 \\
 \hline
 \textbf{Cumulative Distribution Function} &  $1 - e^{-(x/\lambda)^k}$ \\
 \hline
 \textbf{Mean} & $\lambda \Gamma(1 + 1/k)$ \\
 \hline
 \textbf{Median} & $\lambda (ln 2)^{1/k}$ \\
 \hline
 \textbf{Variance} & $\lambda^2 \Big[ \Gamma \Big( 1 + \frac{2}{k} \Big) - \Big( \Gamma \Big( 1 + \frac{1}{k} \Big) \Big)^2 \Big]$ \\
 \hline
 \textbf{Mode} & $\lambda (\frac{k-1}{k})^{1/k}$ \\
 \hline
 \textbf{Moment Generating Function} & $\sum^\infty_n = 0 \frac{t^n \lambda^n}{n!} \Gamma(1 + n/k), k\geq 1$  \\
 \hline
 \textbf{Skewness} & $\frac{\Gamma(1+3/k) \lambda^3 - 3 \mu \sigma^2 - \mu^3}{\sigma^3}$ \\
 \hline
 \textbf{Excess Kurtosis} & $\frac{-6 \Gamma^4_1 + 12 \Gamma^2_1 \Gamma_2 - 3 \Gamma^2_2 - 4 \Gamma_1 \Gamma_3 + \Gamma_4}{[ \Gamma_2 - \Gamma^2_1]^2}$ \\
 \hline
       
  \end{tabular}
  
The Weibull Distribution is a continuous probability distribution (which was named after Swedish mathematician Waloddi Weibull) used to analyse life data, model failure times and access product reliability. 

\section{Student T-Distribution}

\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }& $\nu > 0$ (degrees of freedom) \\
 \hline
 \textbf{Support} & $x \in (- \infty,\infty)$ \\
 \hline
 \textbf{Probability Mass Function} & $\frac{\Gamma (\frac{\nu +1}{2})}{\sqrt{\nu \pi} \Gamma(\frac{\nu}{2})} \Big( 1+ \frac{x^2}{\nu} \Big) ^{- \frac{\nu+1}{2}}$ \\
 \hline
 \textbf{Cumulative Distribution Function} & $\frac{1}{2} + x \Gamma(\frac{\nu+1}{2} ) \frac{_2F_1 (\frac{1}{2} , \frac{\nu + 1}{2} ; \frac{3}{2} ; - \frac{x^2}{\nu} )}{\sqrt{pi \nu } \Gamma(\frac{\nu}{2} )}$  \\
 \hline
 \textbf{Mean} & $0$ if $\nu > 1$ \\ & Undefined otherwise\\ 
 \hline
 \textbf{Variance} & $\frac{\nu}{\nu - 2}$ if $\nu > 2$ \\ & $\infty$ if $1 < \nu \leq 2$ \\ & Undefined otherwise \\
 \hline
 \textbf{Median} &  $0$\\ 
 \hline
 \textbf{Mode} & $0$\\
 \hline
 \textbf{Moment Generating Function} & does not exist \\
 \hline
 \textbf{Skewness} & $0 $ for $ \nu > 3 $ \\ & undefined otherwise \\
 \hline
 \textbf{Excess Kurtosis} & \textbf{TO BE UNDERSTOOD} \\
 \hline
       
  \end{tabular}

The Student's t-distribution was developed by English statistician William Sealy Gosset under the pseudonym "Student". The t-distribution plays a role in a number of widely used statistical analyses, including Student's t-test for assessing the statistical significance of the difference between two sample means, the construction of confidence intervals for the difference between two population means, and in linear regression analysis. Student's t-distribution also arises in the Bayesian analysis of data from a normal family. If we take a sample of $n$ observations from a normal distribution, then the t-distribution with $ \ nu =n-1$ degrees of freedom can be defined as the distribution of the location of the sample mean relative to the true mean, divided by the sample standard deviation, after multiplying by the standardizing term $\sqrt {n}$. In this way, the t-distribution can be used to construct a confidence interval for the true mean.(Student's t-distribution, Wikipedia)

The t-distribution is shaped like the normal distribution (symmetric and bell shaped). However it has heavier tails. The tails become heavier as $\nu$ gets less. At $\nu = \infty$, the student t distribution resembles the normal distribution

\textbf{To find Skewness}
\[ \mu_3= \frac{E(X^3) - 3 \mu \sigma^2 - \mu^3}{ \sigma^3} \]
\\ $E(X^3) = \mu = 0$ when $\nu > 3$
\\ $E(X^3)$ does not exist when $\nu < 3$

\[ \mu_3= 0 \text{ for } \nu > 3 \text{ ,undefined o/w}\]
\textbf{To find Kurtosis}
\[ \mu_4 = \frac{ E(X^4) - 4 \mu E(X^3) + 6 \mu^2 E(X^2) - 3 \mu^4}{ \sigma^4} \]
\\ $E(X^4)$ does not exist when $\nu < 4$



\section{Pareto Distribution}

\begin{tabular}{|c|c|}  %no. of columns
\hline
 \textbf{Parameters }& $x_m > 0$ (scale) \\ & $\alpha >0$ (shape) \\
 \hline
 \textbf{Support} & $x \in [x_m, \infty)$ \\
 \hline
 \textbf{Probability Mass Function} & $\frac{\alpha x_m^\alpha}{x^{\alpha +1}}$ if $x \geq x_m$ \\ & $0$ if $x < x_m$ \\
 \hline
 \textbf{Cumulative Distribution Function} &  $1 - (\frac{x_m}{x})^\alpha$ \\
 \hline
 \textbf{Mean} & $\infty$ if $\alpha \leq 1$ \\ & $\frac{\alpha x_m}{\alpha - 1}$ if $\alpha > 1$ \\ 
 \hline
 \textbf{Variance} & $\infty$ if $\alpha \in (1,2] $ \\ & $\big( \frac{x_m}{\alpha - 1}\big) ^2 \frac{\alpha}{\alpha - 2}$ if $\alpha > 2$\\
 \hline
 \textbf{Median} & $x_m  \sqrt[\alpha]{2}$ \\ 
 \hline
 \textbf{Mode} & $x_m$\\
 \hline
 \textbf{Moment Generating Function} & does not exist \\
 \hline
 \textbf{Skewness} & $\frac{2(1+\alpha)}{(\alpha - 3)} \sqrt{\frac{\alpha - 2}{\alpha}}$\\
 \hline
 \textbf{Excess Kurtosis} & $\frac{6(\alpha^3 +\alpha^2 - 6\alpha -2 )}{\alpha(\alpha - 3)(\alpha - 4)}$ \\
 \hline
       
  \end{tabular}

The Pareto distribution is one of the most commonly found distributions in the real world. (It was first used to describe the distribution of wealth i.e. how a small percentage of the population hold a large percentage of the wealth. Now it is used in description of social, quality control, scientific, geophysical, actuarial, and many other types of observable phenomena) It is a type of power law which is named after the Italian civil engineer, economist, and sociologist Vilfredo Pareto. It is a fat tailed distribution like all other power laws. The most widely known form of the Pareto distribution is that which follows the Pareto principle or "80-20 rule" which states that 80 percent of outcomes are due to 20 percent of causes. Only Pareto distributions with shape value ($\alpha$) of $log_4 5 \approx 1.16$ precisely reflect it. Empirical observation has shown that this 80-20 distribution fits a lot of phenomena, both natural ones and human activities.

\\ \underline{Mean}
\[ E(X) = \int^\infty _{x_{min}} x f(x) dx\]
\[ E(X) = \int^\infty _{x_{min}} x \frac{\alpha x_{min}^\alpha}{x^{\alpha+1}} dx\]
\[ E(X) = \alpha x_{min}^\alpha \int^\infty _{x_{min}} x^{1 - \alpha - 1} dx\]
\[ E(X) = \alpha x_{min}^\alpha \Big[ \frac{x^{- \alpha +1}}{-\alpha + 1} \Big]^\infty_{x_{min}} dx\]
\[ E(X) =  \frac{\alpha x_{min}^\alpha x_{min}^{-\alpha + 1}}{\alpha - 1} \]
\[ E(X) = \frac{\alpha x_{min}}{\alpha - 1} \]
If $\alpha > 1$, $E(X) = \frac{\alpha x_{min}}{\alpha - 1} $ 
\\ If $\alpha \leq 1$, $E(X) = \infty$
\underline{Variance}
\\ $Var(x) = E(X^2) - (E(X))^2$
\[ E(X^2)  = \int^\infty_{x_{min}} x^2 f(x) dx\]
\[ = \int^\infty_{x_{min}} x^2 \frac{\alpha x_{min}^\alpha}{x^{\alpha+1}} dx\]
\[ = \alpha x_{min}^\alpha \int^\infty_{x_{min}} x^{2-\alpha-1} \]
\[ = \alpha x_{min}^\alpha \int^\infty_{x_{min}} x^{1-\alpha} \]
\[ = \alpha x_{min}^\alpha \Big[ \frac{x^{1-\alpha+1}}{1-\alpha+1} \Big]^\infty_{x_{min}} \]
\[ = \alpha x_{min}^\alpha \frac{x_{min}^{2-\alpha}}{\alpha - 2}  \]
\[ = x_{min}^2 \frac{\alpha}{\alpha-2}  \text{ when } \alpha > 2 \]
\[ Var(x) = x_{min}^2 \frac{\alpha}{\alpha-2} - \Big( \frac{\alpha x_{min}}{\alpha - 1} \Big)^2 \]
\[ = x^2_{min} \Big( \frac{\alpha}{\alpha-2} - \frac{\alpha^2}{(\alpha - 1)^2} \Big) \]
\[ = x^2_{min} \Big( \frac{\alpha(\alpha - 1)^2 - \alpha^2(\alpha-2)}{(\alpha - 2)(\alpha - 1)^2} \Big) \]
\[ = x^2_{min} \alpha \Big( \frac{(\alpha - 1)^2 - \alpha(\alpha-2)}{(\alpha - 2)(\alpha - 1)^2} \Big)\]
\[ = x^2_{min} \alpha \Big( \frac{(\alpha^2 + 1 - 2 \alpha - \alpha^2 + 2 \alpha )}{(\alpha - 2)(\alpha - 1)^2} \Big)\]
\[ = x^2_{min} \alpha \Big( \frac{1}{(\alpha - 2)(\alpha - 1)^2} \Big)\]
\[ Var(X) = \frac{x^2_{min} \alpha }{(\alpha - 2)(\alpha - 1)^2} \text{ when } \alpha > 2\]
\[ Var(x) = \infty \text{ when } \alpha \in (1,2] \]
\\
\\ \underline{Higher Moments}
\[ E (X^m) = \int^\infty_{x_{min}} x^m  f(x) dx\]
\[ E (X^m) = \int^\infty_{x_{min}} x^m \frac{\alpha x_{min}^\alpha}{x^{\alpha+1}} dx\]
\[ E (X^m) = \alpha x_{min}^\alpha \int^\infty_{x_{min}} x^{m - \alpha -1} dx\]
\[ E (X^m) = \alpha x_{min}^\alpha \Big[ \frac{x^{m - \alpha -1+1}}{m - \alpha -1+1} \Big]^\infty_{x_{min}} \]
\[ E (X^m) = \alpha x_{min}^\alpha \Big[ \frac{x^{m - \alpha}}{m - \alpha} \Big]^\infty_{x_{min}} \]
\[ E (X^m) = \alpha x_{min}^\alpha \Big[ \frac{x_{min}^{m - \alpha}}{\alpha - m} \Big] \]
\[ E (X^m) = x_{min}^m \frac{\alpha}{\alpha - m}  \]
\\
\\ If $n< \alpha$, $E (X^m) = x_{min}^m \frac{\alpha}{\alpha - m}$.
\\ If $n \geq \alpha $, $E(X^m) = \infty$
\\
\\ .


\textbf{To find Skewness}
\[ \mu_3= \frac{E(X^3) - 3 \mu \sigma^2 - \mu^3}{ \sigma^3} \]

\[\mu_3 = \frac{\frac{\alpha x_m^3}{\alpha - 3} - 3 (\frac{\alpha x_m}{\alpha-1}) \frac{x_m^2 \alpha}{(\alpha - 1)^2 (\alpha -2)} - (\frac{\alpha x_m}{\alpha - 1})^3}{\frac{x_m^2 \alpha}{(\alpha - 1)^2 (\alpha - 2)}\frac{x_m}{(\alpha - 1)} \sqrt{\frac{\alpha}{\alpha - 2}}}\]

\[\mu_3 = \frac{\frac{\alpha x_m^3}{\alpha - 3} - 3 \frac{\alpha^2 x_m^3}{(\alpha-1)^3(\alpha -2)} - \frac{\alpha^3 x_m^3}{(\alpha - 1)^3}}{\frac{x_m^3 \alpha}{(\alpha - 1)^3(\alpha - 2)} \sqrt{\frac{\alpha}{\alpha - 2}}}\]

\[\mu_3 = \frac{(\alpha - 1)^3(\alpha - 2) - 3 \alpha (\alpha - 3) - \alpha^2 (\alpha - 2)(\alpha - 3)}{(\alpha - 3) \sqrt{\frac{\alpha}{\alpha - 2}}}\]

\[\mu_3 = \frac{\alpha^4 - 3 \alpha^3 +3 \alpha^2 - \alpha - 2\alpha^3 + 6 \alpha^2 - 6\alpha + 2 - 3 \alpha^2 + 9 \alpha - \alpha^4 + 2 \alpha^3 + 3 \alpha^3 - 6 \alpha^2 }{(\alpha - 3) \sqrt{\frac{\alpha}{\alpha - 2}}}\]

\[\mu_3 = \frac{2(\alpha +1)}{\alpha-3} \sqrt{\frac{\alpha - 2}{\alpha}}\]


\textbf{To find Kurtosis}
\[ \mu_4 = \frac{ E(X^4) - 4 \mu E(X^3) + 6 \mu^2 E(X^2) - 3 \mu^4}{ \sigma^4} \]

\[\mu_4 = \frac{\frac{\alpha x_m^4}{\alpha - 4} - \frac{4 \alpha x_m}{\alpha - 1} \frac{\alpha x_m^3}{(\alpha - 3)} + 6 (\frac{\alpha x_m}{\alpha - 1})^2 \frac{\alpha x_m^2}{(\alpha - 2)} - 3(\frac{\alpha x_m}{\alpha - 1})^4 }{\frac{x_m^4 \alpha^2}{(\alpha - 1)^4 (\alpha -2)^2}}\]

\[\mu_4 = \frac{\frac{\alpha x_m^4}{\alpha - 4} - \frac{4 \alpha^2 x_m^4}{(\alpha - 1)(\alpha - 3)} + 6 \frac{\alpha^3 x_m^4}{(\alpha - 1)^2 (\alpha - 2)} - 3\frac{\alpha^4 x_m^4}{(\alpha - 1)^4} }{\frac{x_m^4 \alpha^2}{(\alpha - 1)^4 (\alpha -2)^2}}\]

\[\mu_4 = \frac{(\alpha -1)^4 (\alpha - 2)(\alpha - 3) - 4 \alpha(\alpha - 1)^3(\alpha - 2)(\alpha - 4) + 6 \alpha^2 (\alpha - 1)^2(\alpha - 3)(\alpha - 4) - 3 \alpha^3(\alpha - 2)(\alpha - 3)(\alpha - 4)}{\frac{\alpha (\alpha - 3)(\alpha - 4)}{(\alpha - 2)}}\]

\[\mu_4 = \frac{(9 \alpha^2 + 3 \alpha + 6)(\alpha - 2)}{\alpha (\alpha - 3)(\alpha - 4)} \text{when $\alpha > 4$}\]

\textbf{Excess Kurtosis} 
\[\text{Excess kurtosis }= \frac{(9 \alpha^2 + 3 \alpha + 6)(\alpha - 2)}{\alpha (\alpha - 3)(\alpha - 4)} - 3\]

\[\text{Excess kurtosis }= \frac{9 \alpha^3 - 15 \alpha^2 - 12 - 3 \alpha (\alpha - 3)(\alpha - 4) }{\alpha (\alpha - 3)(\alpha - 4)}\]

\[\text{Excess kurtosis }= \frac{9 \alpha^3 - 15 \alpha^2 - 12 - 3 \alpha^3 + 21 \alpha^2 - 36 \alpha}{\alpha (\alpha - 3)(\alpha - 4)}\]

\[\text{Excess kurtosis }= \frac{6(\alpha^3 + \alpha^2 - 6 \alpha - 2) }{\alpha (\alpha - 3)(\alpha - 4)} \text{when $\alpha > 4$}\]
\\
\\
\newpage
\section{Power Law}
\\$ f(x) = k x^{-\alpha}$
\\
\\ \underline{Finding the value of k}
\[\int^\infty_{x_{min}} f(x) dx = 1 \]
\[ \int^\infty_{x_{min}} kx^{-\alpha} dx = 1\]
\[ k \int^\infty_{x_{min}} x^{-\alpha} dx = 1\]
\[ k \Big[ \frac{x^{-\alpha +1}}{-\alpha + 1} \Big]^\infty_{x_{min}} = 1\]
\[ k \frac{x_{min}^{-\alpha +1}}{\alpha - 1} = 1\]
\[ k = \frac{\alpha - 1}{x_{min}^{1 -\alpha}}\]
\\
\\
\\ \underline{Mean} 
\\ A well defined mean exists $ \in [x_{min}, \infty )$ iff $\alpha \geq 2$ 
\[ E(X) = \int^\infty_{x_{min}} x f(x) dx \]
\[ = k \int^\infty_{x_{min}} x \frac{1}{x^\alpha} dx\]
\[ = k \int^\infty_{x_{min}} x^{1-\alpha} dx\]
\[ = \frac{\alpha - 1}{x_{min}^{1 -\alpha}} \Big[ \frac{x^{2 -\alpha}}{2 - \alpha } \Big]^\infty_{x_{min}} \]
\[ = \frac{\alpha - 1}{x_{min}^{1 -\alpha}} \Big[ \frac{x_{min}^{2 -\alpha}}{\alpha -2 } \Big] \]
\[ = \frac{\alpha - 1}{\alpha - 2} x_{min}\]
\\
\\ If $\alpha = 2$ \[ E(X) = k \int^\infty_{x_{min}} \frac{1}{x} dx \]
$log(\infty)$ does not exist, $\therefore$ $\alpha \neq 2$
\\
\\ If $\alpha < 2$, the integral does not converge and hence does not exist.
\newpage
\underline{Variance} 
\\ $Var(x) = E(X^2) - (E(X))^2$
\\ \[ E(X)^2  = \int^\infty_{x_{min}} x^2 f(x) dx\]
\[  =  \int^\infty_{x_{min}} x^2 \frac{\alpha - 1 }{x_{min}^{1-\alpha}}x^{-\alpha} dx\]
\[  =  \frac{\alpha - 1 }{x_{min}^{1-\alpha}} \int^\infty_{x_{min}} x^{2 -\alpha} dx\]
\[  =  \frac{\alpha - 1 }{x_{min}^{1-\alpha}} \Big[ \frac{x^{2 -\alpha+1}}{2- \alpha +1} \Big]^\infty_{x_{min}} \]
\[  =  \frac{\alpha - 1 }{x_{min}^{1-\alpha}} \Big[ \frac{x^{3 -\alpha}}{3 - \alpha } \Big]^\infty_{x_{min}} \]
\[  =  \frac{\alpha - 1 }{x_{min}^{1-\alpha}} \Big[ \frac{x_{min}^{3 -\alpha}}{\alpha - 3 } \Big] \]
\[  =  \frac{\alpha - 1 }{\alpha - 3} x_{min}^{3 -\alpha - 1+ \alpha}\]
\[ = \frac{\alpha - 1 }{\alpha - 3} x_{min}^2\]
\\
\[ Var(X) = E(X^2) - (E(X))^2\]
\[ Var(X) = \frac{\alpha - 1 }{\alpha - 3} x_{min}^2 - (\frac{\alpha - 1}{\alpha - 2} x_{min})^2\]
\[ Var(X) = x_{min}^2 \Big( \frac{\alpha -1}{\alpha - 3} - (\frac{\alpha - 1}{\alpha - 2}) ^2\Big)\]
\\ 
\\ 
\\ \underline{Higher Moments}
\[ E(X^m) = \int^\infty_{x_{min}} x^m f(x) dx \] 
\[ E(X^m) = k \int^\infty_{x_{min}} x^m x^{-\alpha} dx\]
\[ E(X^m) = \frac{\alpha - 1}{x_{min}^{1 -\alpha}} \int^\infty_{x_{min}} x^{m-\alpha} dx\]
\[ E(X^m) = \frac{\alpha - 1}{x_{min}^{1 -\alpha}} \Big[ \frac{x^{m-\alpha + 1}}{m- \alpha +1} \Big]^\infty _{x_{min}} \]
\[ E(X^m) = \frac{\alpha - 1}{x_{min}^{1 -\alpha}} \Big[ \frac{x_{min}^{-\alpha + 1} x_{min}^m}{\alpha -m  -1} \Big] \]
\[ E(X^m) = \frac{\alpha - 1}{\alpha - m - 1} x_{min}^m\]
\newpage
\begin{center}
    \Large{\textbf{Bibliography}}
\end{center}

\begin{enumerate}
    \item Wikipedia (2022) Fat-tailed Distribution. Available at: \url{https://en.wikipedia.org/wiki/Fat-tailed_distribution}
    \item Taleb, N.N. (2022) 'MINI-LESSON 1: Breaking down intuitively the concept of standard deviation. Why pple don't get it.'. N N Taleb’s Probability Moocs, . Available at: \url{https://www.youtube.com/watch?v=oMl-SbuQUYc&list=PLMV8UXQuOWKPAIjvnyMN2317LHF3ydvnG&index=8} (Accessed: date).
    
    \item Taleb, N.N. (2022) 'MINI-LESSON 8: Power Laws(maximally simplified'. N N Taleb’s Probability Moocs, . Available at: \url{https://www.youtube.com/watch?v=oMl-SbuQUYc&list=PLMV8UXQuOWKPAIjvnyMN2317LHF3ydvnG&index=8} (Accessed: date).
    \item Taleb, N.N. (2022) 'MINI-LESSON 2: Fat Tails, a Very, Very Introductory Presentation.'. N N Taleb’s Probability Moocs, . Available at: \url{https://www.youtube.com/watch?v=t7Fr6iGhmBM&list=PLMV8UXQuOWKPAIjvnyMN2317LHF3ydvnG&index=2} (Accessed: date).
    \item Taleb, N.N. (2022) 'MINI-LESSON 3: The Law of Large Numbers. A very intuitive introduction.'. N N Taleb’s Probability Moocs, . Available at: \url{https://www.youtube.com/watch?v=zyBXNIskQK0&list=PLMV8UXQuOWKPAIjvnyMN2317LHF3ydvnG&index=3} (Accessed: date).
    \item Glen S. "Fat Tail Distribution: Definition, Examples" StatisticsHowTo.com: Elementary Statistics for the rest of us! Available at: \url{https://www.statisticshowto.com/fat-tail-distribution/}
    \item Wikipedia (2022) Normal Distribution. Available at: \url{https://en.wikipedia.org/wiki/Normal_distribution} (Accessed: date)
    \item Wikipedia (2022) Binomial Distribution. Available at: \url{https://en.wikipedia.org/wiki/Binomial_distribution} (Accessed: date)
    \item Wikipedia (2022) Bernoulli Distribution. Available at: \url{https://en.wikipedia.org/wiki/Bernoulli_distribution} (Accessed: date)
    \item Wikipedia (2022) Gamma Distribution. Available at: \url{https://en.wikipedia.org/wiki/Gamma_distribution} (Accessed: date)
    \item Wikipedia (2022) Exponential Distribution. Available at: \url{https://en.wikipedia.org/wiki/Exponential_distribution} (Accessed: date)
    \item Wikipedia (2022) Logarithmic Distribution. Available at: \url{https://en.wikipedia.org/wiki/Logarithmic_distribution} (Accessed: date)
    \item Wikipedia (2022) Cauchy Distribution. Available at: \url{https://en.wikipedia.org/wiki/Cauchy_distribution} (Accessed: date)
    \item Wikipedia (2022) Levy Distribution. Available at: \url{https://en.wikipedia.org/wiki/L%C3%A9vy_distribution} (Accessed: date)
    \item Wikipedia (2022) Student's t-distribution. Available at: \url{https://en.wikipedia.org/wiki/Student%27s_t-distribution} (Accessed: date)
    \item Wikipedia (2022) Pareto Distribution. Available at: \url{https://en.wikipedia.org/wiki/Pareto_distribution} (Accessed: date)
    \item Wikipedia (2022) Power Law. Available at: \url{https://en.wikipedia.org/wiki/Power_law} (Accessed: date) 
    \item Evans, M & Hastings, Nicholas & Peacock, Brian. (2000). Statistical Distributions, Third Edition. Measurement Science and Technology. Available at: \url{http://www.ru.ac.bd/stat/wp-content/uploads/sites/25/2019/03/201_06_Forbes_-Statistical-Distributions-Fourth-Edition-Wiley-2011.pdf}  
    
\end{enumerate}






 


\end{document}
